### Andrew Ng: [Machine Learning](https://www.coursera.org/learn/machine-learning/home/welcome) 
|在线课堂|完成时间|代码地址|
|:----------|:----------|:---------:|
|Introduction|2017/03/11||
|Linear Regression with Multiple Variables|2017/04/10|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex1)|
|Logistic Regression|2017/05/13| [代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex2)|
|Neural Networks: Representation|2017/05/15|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex3)|
|Neural Networks: Learning|2018/08/13|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex4)|
|Advice for Applying Machine Learning|2018/08/16|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex5)|
|Support Vector Machines|2018/08/19|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex6)|
|Unsupervised Learning|2018/08/20|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex7)|
|Anomaly Detection & Recommender Systems|2018/08/23|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Machine%20Learning/machine-learning-ex8)|
|Large Scale Machine Learning|2018/08/23||
|Application Example: Photo OCR|2018/08/24||

### Andrew Ng: [Deep Learning Specialization - Neural Networks and Deep Learning](https://www.coursera.org/learn/machine-learning/home/welcome) 
|在线课堂|完成时间|代码地址|
|:----------|:----------|:---------:|
|Introduction to deep learning|2018/08/25|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/1-Neural%20Networks%20and%20Deep%20Learning/Week%201)|
|Neural Networks Basics|2018/08/28|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/1-Neural%20Networks%20and%20Deep%20Learning/Week%202)|
|Shallow neural networks|2018/08/30|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/1-Neural%20Networks%20and%20Deep%20Learning/Week%203)|
|Deep Neural Networks|2018/09/06|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/1-Neural%20Networks%20and%20Deep%20Learning/Week%204)|

### Andrew Ng: [Deep Learning Specialization - Improving Deep Neural Networks](https://www.coursera.org/learn/machine-learning/home/welcome) 
|在线课堂|完成时间|代码地址|
|:----------|:----------|:---------:|
|Practical aspects of Deep Learning|2018/09/10|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/2-Improving%20Deep%20Neural%20Networks/Week%201)|
|Optimization algorithms|2018/09/12|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/2-Improving%20Deep%20Neural%20Networks/Week%202)|
|Hyperparameter tuning, Batch Normalization and Programming Frameworks|2018/09/15|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/2-Improving%20Deep%20Neural%20Networks/Week%203)|

### Andrew Ng: [Deep Learning Specialization - Structuring Machine Learning Projects](https://www.coursera.org/learn/machine-learning/home/welcome) 
|在线课堂|完成时间|代码地址|
|:----------|:----------|:---------:|
|ML Strategy (1)|2018/09/17|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/3-Structuring%20Machine%20Learning%20Projects/Week%201)|
|ML Strategy (2)|2018/09/17|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/3-Structuring%20Machine%20Learning%20Projects/Week%202)|

### Andrew Ng: [Deep Learning Specialization - Convolutional Neural Networks](https://www.coursera.org/learn/machine-learning/home/welcome) 
|在线课堂|完成时间|代码地址|
|:----------|:----------|:---------:|
|Foundations of Convolutional Neural Networks|2018/09/21|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/4-Convolutional%20Neural%20Networks/Week%201)|
|Deep convolutional models: case studies|2018/09/25|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/4-Convolutional%20Neural%20Networks/Week%202)|
|Object detection|2018/09/28|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/4-Convolutional%20Neural%20Networks/Week%203)|
|Special applications: Face recognition & Neural style transfer|2018/10/29|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/4-Convolutional%20Neural%20Networks/Week%204)|

### Andrew Ng: [Deep Learning Specialization - Sequence Models](https://www.coursera.org/learn/machine-learning/home/welcome) 
|在线课堂|完成时间|代码地址|
|:----------|:----------|:---------:|
|Recurrent Neural Networks|2018/11/18|[代码](https://github.com/daleiyang/NLP-MachineLearning/tree/master/Deep%20Learning%20Specialization/5-%20Sequence%20Models/Week%201)|

### 李航 统计学习方法
|章节|完成时间|示例代码|
|:----------|:---------:|:----------|
|[第01章] 统计学习方法概论|2017/06/15|无
|[第02章] 感知机|2017/07/20|LiHang\Ch02\下的Perceptron.py和Perceptron2.py
|[第04章] 朴素贝叶斯法|2017/06/17|LiHang\Ch04\下的Bayes.py
|[第09章] EM算法及其推广|2017/07/03|LiHang\Ch09\下的em.py和test.py
|[第10章] 隐马尔可夫模型|2017/06/27|LiHang\Ch10\下的hmm.py和test.py

###  自然语言处理
|阅读源码|完成时间|资料地址|心得|
|:----------|:----------|:----------|:----------|
|基于词表的分词程序friso|2017/07/18|Segmentation\friso-master|1|
|基于结构化平均感知机的词性标注器|2017/07/24|POSTagging\AveragedPerceptron|2|
|基于结构化平均感知机的分词器|2017/08/02|Segmentation\结构化平均感知机的分词器|3|
|神经网络正向反向传播算法|2017/08/18|BP|4|
|word2vector|2017/09/07|w2v|5|
1. 这是一份基于词表的分词器源码，使用 MMSEG 消歧。体会如下：
   * 作者 C 功底深厚，实现了 string、动态数组、双向链表、基于 buckte 的 hash 表等基础数据结构。
   * 精细的内存控制手法值得学习。
   * FMM 和 MMSEG 算法代码清晰易懂，易于做二次开发，解 bad case。
   * 每种数据结构都有相对应的测试程序，稍加改动 MakeFile 后，就能用 make debug、gdb 单步。
   * 数字英文、英文数字组合以及二次切分的逻辑复杂，以后有需要的话再仔细研究。

2. 这是一份基于感知机的词性标注器源码。体会如下：
   * 用结构化平均感知机这种判别式模型学习特征的概率，然后用 HMM 这种生成式模型生成结果，这是很先进的方法。
   * 词性标注的特征抽取方法。
   * Collins 论文中的算法是如何在结构化平均感知机的学习算法中实现的。

3. 这是一份基于感知机的分词源码。代码短小精悍，技法纯熟。体会如下：
   * 用结构化平均感知机这种判别式模型学习特征的概率，然后用 HMM 这种生成式模型生成结果，这是很先进的方法。
   * 分词，命名实体识别采用的特征抽取方法、词性标注的特征抽取方法其实大同小异。
   * 如何实现 Collins 论文中的学习算法、面对 L1 , L2 正则化时如何变通 。
   * 使用 Bakeoff05 的 MSR 训练、测试语料，经过5轮训练，无正则化处理，F 值在 0.9472。
   * 存在的问题是 L1 , L2 的学习率需要调整，模型剪裁也还没实验。
   * 体会到 python 用很短的代码就能够完成复杂功能，但是训练起来很慢。
   * 很想做一份 C 的实现，和清华分词比比效果，以后有时间再做吧。

4. 下面两篇文章讲解了单层、多层神经网络的前向、反向传播公式推导和代码实现。
   * hankcs 文章推导了单隐藏层的公式，清晰、简洁，多看几遍就能理解，其中的梯度更新公式和 BP/source/bpnn.py 十分匹配，BP/source/code_recognizer.py 是使用BP/source/bpnn.py 的例子。
   * Demystifying Deep Convolutional Neural Networks 文章推导了多隐藏层的公式，图文并茂的讲解了原理，并且用Matlab编码实现，强烈推荐。

5. w2v Principle and Source Code Analysis         
   *  word2vec is a short 700 lines of code, but the depth of the programing skill is breathtaking. The author, Tomas Mikolov, is supposed to be an algorithm contest player, judging by his code style. Programming skills that can be learned from this code are: Handling of input parameters, Accelerate sigmoid function calculations with a segmented lookup table, Weighted sampling, Tips for reading files, Dynamically control the size of the hash table, Building Huffman trees, Splitting files by thread, Random number generation methods, Randomizing window size, Adaptive learning rate.
   *  "Mathematical principles in word2vec"(https://github.com/daleiyang/NLP-MachineLearning/blob/master/w2v/word2vec_%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3.pdf) This article explains in detail the machine learning principles used in this code: Logistic regression, language models, optimization goals, formula derivation, gradient update, a variety of detailed techniques are available, formula derivation and coding consistency is very high. Understand w2v, read this article is enough.
   *  
5. 我用一个月的时间，找材料、读论文、看代码、找到了最好的训练方法、结合适合的对齐方式、活用w2v的能力，使得句子相似度匹配的精确度有了大幅度提高。具体项目细节、实现方法，请参考["短句子相似度计算引擎"](https://github.com/daleiyang/Similarity)。
   *  word2vec 短短700行代码，博大精深，令人叹为观止。作者 Tomas Mikolov 应该是 OIer 出身，程序写法十分竞赛风格。从代码中学到的编程技巧有：处理参数输入、用分段查表的方法加速 sigmoid 函数计算、带权采样、读文件的技巧、动态控制哈希表大小、建哈夫曼树、按线程拆分文件、随机数生成方法、随机窗口大小、自适应学习率。这些技巧都可以直接拿来用。
   * <<word2vec 中的数学原理详解>> 讲解细致入微：逻辑回归、语言模型、优化目标、公式推导、梯度更新、各种细节技巧面面俱到，公式推导和代码实现一致度很高。理解w2v，看懂这篇文章就够了。
   * <<How to Generate a Good Word Embedding?>> 通过大量实验，对比了各种词向量和不同训练参数的效果，并推荐了最佳选择，可以直接拿来用。

### 可汗学院公开课：统计学
|章节|完成时间|
|:----------|:----------|
|01到28节|2017/06/07|
|29到50节|2017/06/08|
|51到55节|2017/06/09|
|56到61节|2017/06/11|
|62到85节|2017/06/12|

### MIT 18.06：Linear Algebra
|课堂讲课|完成时间|习题课|完成时间|
|:----------|:----------|:----------|:---------:|
|[第1集] 方程组的几何解释|2017/02/13|[第1集] 线性代数中的几何学|2017/02/13|
|[第2集] 矩阵消元|2017/02/14|[第3集] 矩阵的消去法|2017/02/14|
|[第3集] 乘法和逆矩阵|2017/02/15|[第4集] 逆矩阵|2017/02/15|
|[第4集] A的LU分解|2017/02/16|[第5集] LU分解|2017/02/16|
|[第5集] 转置-置换-向量空间R |2017/02/18|[第6集] 三维空间的子空间|2017/02/18|
|[第6集] 列空间和零空间|2017/02/18|[第7集] 向量子空间|2017/02/18|
|[第7集] 求解Ax=0：主变量、特解|2017/02/19|[第8集] 解Ax=0|2017/02/19|
|[第8集] 求解Ax=b：可解性和解的结构|2017/02/23|[第9集] 解Ax=b|2017/02/23|

### MIT 18.01：Single Variable Calculus
|课堂讲课|完成时间|习题课|完成时间|
|:----------|:----------|:----------|:---------:
|[第1集] 导数和变化率|2017/03/04|[第1集] 课程简介|2017/03/04|
|[第2集] 极限和连续|2017/03/04|[第2集] 导数的定义 [第3集] 导数的图像|2017/03/04|
|[第3集] 求导四则运算及三角函数导数|2017/03/05|[第4集] 分段函数的光滑化|2017/03/05|
|[第4集] 链式法则及高阶导数|2017/03/05|||
|[第5集] 隐函数微分法和逆函数导数 |2017/03/05|||
